
package com.test.streaming

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.spark.streaming._
import org.apache.spark.streaming.twitter._
import org.apache.spark.streaming.StreamingContext._
import org.apache.log4j.Level
import Utilities._
import org.joda.time.DateTime._
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.storage.StorageLevel
import java.util.regex.Pattern
import java.util.regex.Matcher
import com.datastax.spark.connector._
import com.datastax.spark.connector.streaming._
import StreamingContext._ 
import org.apache.spark.SparkContext._
import com.twitter.jsr166e;
import java.util.Map;
import java.util.Set;
import java.io.Serializable;
import java.util.concurrent.atomic.AtomicLong;
import java.io.Serializable;
import com.datastax.spark.connector.writer._
import com.datastax.spark.connector.cql.{ColumnDef, RegularColumn, TableDef}
import com.datastax.spark.connector.types.IntType
import com.datastax.spark.connector.types._
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.dstream
import java.net.InetAddress
import com.datastax.spark.connector.cql.CassandraConnectorConf.CassandraSSLConf
import com.datastax.spark.connector.cql._
import com.datastax.spark.connector.mapper.ColumnMapper
import com.datastax.spark.connector.rdd.partitioner.{CassandraPartitionedRDD, ReplicaPartitioner}
import com.datastax.spark.connector.rdd.reader._
import com.datastax.spark.connector.rdd._
import com.datastax.spark.connector.writer.{ReplicaLocator, _}
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import com.datastax.spark.connector.cql.CassandraConnector._
import scala.reflect.ClassTag

/** Simple application to listen to a stream of Tweets and print them out */
object PrintTweets3 {
 
  /** Our main function where the action happens */
  def main(args: Array[String]) {

    // Configure Twitter credentials using twitter.txt
    setupTwitter()
    
    val sc = new SparkConf()
       .set("spark.cassandra.connection.host", "127.0.0.1")
       .setMaster("local[*]")
       .setAppName("PrintTweets3")
       .set("spark.cassandra.connection.port", "9042");
    
    // Create the context with a 10 second batch size
    val	ssc	= new StreamingContext(sc, Seconds(2))
    val	stream	= TwitterUtils.createStream(ssc, None, Nil,	storageLevel	= StorageLevel.MEMORY_ONLY_SER_2)
    // Recompute the top hashtags every 1 second
    val slideInterval = new Duration(2 * 1000)

    // Compute the top hashtags for the last 5 seconds
    val windowLength = new Duration(2 * 1000)

    // Wait this many seconds before stopping the streaming job
    val timeoutJobLength = 100 * 1000
    
    setupLogging()
    
    CassandraConnector(sc).withSessionDo { session =>
    session.execute(s"CREATE KEYSPACE IF NOT EXISTS demo_ks WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 3 }")
    session.execute(s"Drop table demo_ks.hashtags")
    session.execute(s"CREATE TABLE IF NOT EXISTS demo_ks.hashtags (hashtag text, interval text, mentions counter, PRIMARY KEY((hashtag),interval))WITH CLUSTERING ORDER BY (interval DESC)")
    session.execute(s"CREATE TABLE IF NOT EXISTS demo_ks.hashtags1 (hashtag text, interval text, mentions counter, PRIMARY KEY((hashtag),interval))WITH CLUSTERING ORDER BY (interval DESC)")
    }
    val hashTags = stream.map(_.getText)
    //.filter(_.startsWith("#Trump"))
    //.flatMap(_.split(" "))
    //val	hashTags	=	stream.flatMap(tweet => tweet.getText.toLowerCase.split("	").filter(tags.contains(Seq("#Trump", "#POTUS"))))
    val	tagCounts	=	hashTags.map((_, 1)).reduceByKeyAndWindow((x: Int, y: Int) => x + y, windowLength, slideInterval)
    
    val	tagCountsAll	=	tagCounts.map{case (tag,	mentions) => (tag,	mentions, "ALL")}
    //val	tagCountsByDay	=	tagCounts.map{case (tag,	mentions) => (tag,	mentions, DateTime.now.toString("yyyyMMdd"))}
    tagCountsAll.saveToCassandra("demo_ks", "hashtags", SomeColumns("hashtag", "mentions", "interval"))
    tagCountsAll.saveToCassandra("demo_ks", "hashtags1", SomeColumns("hashtag", "mentions", "interval"))
    //tagCountsAll.foreachRDD{rdd => rdd.deleteFromCassandra("demo_ks", "hashtags1")}//, columns=None)}//, SomeColumns("hashtag", "mentions", "interval"))
    //val rdd = ssc.cassandraTable("demo_ks", "hashtags").deleteFromCassandra("demo_ks", "hashtags")//, columns = None) 
       
    ssc.start()	
    ssc.awaitTermination()	
    
    // Create a DStream from Twitter using our streaming context
    //val tweets = TwitterUtils.createStream(ssc, None)
    
  }
}
